# main.py ‚Äì PHI√äN B·∫¢N HO√ÄN CH·ªàNH (sau khi fix l·ªói startup + th·ª© t·ª± prompt)
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import os
from pathlib import Path
import uuid
import json
from functools import lru_cache
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# ==========================================================
# APP + CORS
# ==========================================================
app = FastAPI(title="Shop C·∫ßu L√¥ng AI ‚Äì Linh Nh·ªõ L√¢u, Si√™u Th√¢n Thi·ªán")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================================
# CONFIG
# ==========================================================
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-72fc18a871824e58a910a499f281512c")
DB_FOLDER = Path("faiss_shopcaulong")
DB_FOLDER.mkdir(exist_ok=True)
HISTORY_FILE = Path("chat_history.json")  # L∆∞u l·ªãch s·ª≠ chat vƒ©nh vi·ªÖn

vectorstore: FAISS | None = None
qa_chain = None
qa_chain_fallback = None

store: Dict[str, ChatMessageHistory] = {}
product_doc_ids = {}

# ==========================================================
# L∆ØU & LOAD L·ªäCH S·ª¨ CHAT
# ==========================================================
def load_history():
    global store
    if HISTORY_FILE.exists():
        try:
            data = json.loads(HISTORY_FILE.read_text(encoding="utf-8"))
            for session_id, messages in data.items():
                history = ChatMessageHistory()
                for msg in messages:
                    if msg["type"] == "human":
                        history.add_message(HumanMessage(content=msg["content"]))
                    else:
                        history.add_message(AIMessage(content=msg["content"]))
                store[session_id] = history
            print(f"ƒê√£ kh√¥i ph·ª•c l·ªãch s·ª≠ c·ªßa {len(store)} phi√™n chat")
        except Exception as e:
            print("L·ªói load l·ªãch s·ª≠ chat:", e)

def save_history():
    data = {}
    for session_id, history in store.items():
        data[session_id] = [
            {"type": "human" if isinstance(m, HumanMessage) else "ai", "content": m.content}
            for m in history.messages
        ]
    try:
        HISTORY_FILE.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception as e:
        print("L·ªói l∆∞u l·ªãch s·ª≠ chat:", e)

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# ==========================================================
# EMBEDDING & LLM
# ==========================================================
@lru_cache()
def get_embedding():
    openai_key = os.getenv("OPENAI_API_KEY")
    if openai_key:
        try:
            print("üîπ ƒêang s·ª≠ d·ª•ng OpenAI Embeddings (text-embedding-3-small)...")
            return OpenAIEmbeddings(
                model="text-embedding-3-small",
                api_key=openai_key,
                max_retries=2,
            )
        except Exception as e:
            print("L·ªói OpenAI Embedding, chuy·ªÉn qua HuggingFace:", e)
    print("üîπ Fallback sang HuggingFace Embedding (multilingual-e5-small)")
    return HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-small",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

@lru_cache()
def get_llm():
    print("üîπ Kh·ªüi t·∫°o DeepSeek LLM...")
    return ChatOpenAI(
        model="deepseek-chat",
        temperature=0.3,
        max_tokens=600,
        api_key=DEEPSEEK_API_KEY,
        base_url="https://api.deepseek.com",
        timeout=20,
        max_retries=2,
    )

# ==========================================================
# DATA MODELS
# ==========================================================
class SizeVariant(BaseModel):
    size: str
    stock: int

class ColorVariant(BaseModel):
    color: str
    imageUrls: List[str] = []
    sizes: List[SizeVariant]

class Product(BaseModel):
    id: int
    name: str
    description: str = ""
    price: float
    discountPrice: Optional[float] = None
    stock: int
    brandName: str
    categoryName: str
    isFeatured: bool = False
    details: List[Dict[str, Any]] = []
    colorVariants: List[ColorVariant] = []

# ==========================================================
# T·∫†O DOCUMENTS T·ª™ PRODUCT
# ==========================================================
def create_product_documents(product: Product) -> List[Document]:
    price = product.discountPrice or product.price
    product_id = product.id
    product_url = f"http://localhost:3000/product/{product_id}"
    docs = []

    # Chunk t·ªïng quan
    info_text = f"{product.name} {product.brandName} gi√° {price:,.0f}ƒë"
    if product.description:
        info_text += f". {product.description}"
    info_text += f" Xem chi ti·∫øt: {product_url}"
    docs.append(Document(
        page_content=info_text,
        metadata={
            "product_id": product_id,
            "type": "full_info",
            "url": product_url
        }
    ))

    # Chunk bi·∫øn th·ªÉ m√†u/size
    for cv in product.colorVariants or []:
        color = (cv.color or "kh√¥ng m√†u").strip()
        available_sizes = [s.size for s in (cv.sizes or []) if s.stock > 0]
        if available_sizes:
            sizes_str = ", ".join(available_sizes)
            variant_text = f"{product.name} m√†u {color} c√≥ size {sizes_str} gi√° {price:,.0f}ƒë c√≤n h√†ng. Xem chi ti·∫øt: {product_url}"
            docs.append(Document(
                page_content=variant_text,
                metadata={
                    "product_id": product_id,
                    "type": "variant",
                    "color": color,
                    "url": product_url
                }
            ))
    return docs

# ==========================================================
# X√ìA CHUNKS C·ª¶A S·∫¢N PH·∫®M
# ==========================================================
def delete_product_chunks(product_id: int) -> int:
    if not vectorstore or vectorstore.index.ntotal == 0:
        return 0
    if product_id in product_doc_ids:
        ids_to_delete = product_doc_ids[product_id]
        vectorstore.delete(ids_to_delete)
        del product_doc_ids[product_id]
        return len(ids_to_delete)

    # Fallback
    ids_to_delete = []
    for i in range(vectorstore.index.ntotal):
        doc_id = vectorstore.index_to_docstore_id[i]
        doc = vectorstore.docstore.search(doc_id)
        if isinstance(doc, Document) and doc.metadata.get("product_id") == product_id:
            ids_to_delete.append(doc_id)
    if ids_to_delete:
        vectorstore.delete(ids_to_delete)
    return len(ids_to_delete)

# ==========================================================
# PROMPTS (ƒë·ªãnh nghƒ©a tr∆∞·ªõc khi build chain)
# ==========================================================
template_with_products = """
B·∫°n l√† Linh ‚Äì nh√¢n vi√™n t∆∞ v·∫•n c·ªßa Shop C·∫ßu L√¥ng Pro.
D·ªÆ LI·ªÜU S·∫¢N PH·∫®M TRONG KHO:
{context}
L·ªäCH S·ª¨ H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{history}
C√ÇU H·ªéI C·ª¶A KH√ÅCH:
{question}
QUY T·∫ÆC TR·∫¢ L·ªúI:
1. Ch·ªâ d√πng th√¥ng tin t·ª´ "D·ªÆ LI·ªÜU S·∫¢N PH·∫®M TRONG KHO"
2. B·∫Øt bu·ªôc d√πng ƒë√∫ng link c√≥ trong d·ªØ li·ªáu: <a href="..." target="_blank">xem chi ti·∫øt</a>
3. G·ª£i √Ω t·ªëi ƒëa 3 s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t
4. G·ªôp m√†u/size c·ªßa c√πng s·∫£n ph·∫©m
5. Gi·ªçng ƒëi·ªáu th√¢n thi·ªán, t·ª± nhi√™n, h·∫°n ch·∫ø emoji
6. C√≥ th·ªÉ ƒë∆∞a th√™m 1-2 c√¢u h·ªèi g·ª£i √Ω ƒë·ªÉ kh√°ch h·ªèi ti·∫øp
TR·∫¢ L·ªúI:
"""

# template_fallback = """
# B·∫°n l√† Linh ‚Äì tr·ª£ l√Ω AI th√¥ng minh c·ªßa Shop C·∫ßu L√¥ng Pro.
# L·ªäCH S·ª¨ H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
# {history}
# C√ÇU H·ªéI C·ª¶A KH√ÅCH:
# {question}
# T√åNH HU·ªêNG: Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p trong kho d·ªØ li·ªáu.
# QUY T·∫ÆC TR·∫¢ L·ªúI:
# 1. B·∫Øt ƒë·∫ßu b·∫±ng th√¥ng b√°o: "‚ö†Ô∏è *Linh ch∆∞a t√¨m th·∫•y s·∫£n ph·∫©m n√†y trong gian h√†ng hi·ªán t·∫°i.*"
# 2. SAU ƒê√ì v·∫´n c·ªë g·∫Øng tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ c·∫ßu l√¥ng
# 3. N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn c·∫ßu l√¥ng, v·∫´n tr·∫£ l·ªùi nh∆∞ng th√™m: "üí° *Th√¥ng tin n√†y mang t√≠nh tham kh·∫£o, c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%*"
# 4. Gi·ªØ gi·ªçng ƒëi·ªáu th√¢n thi·ªán, h·ªØu √≠ch
# 5. G·ª£i √Ω kh√°ch li√™n h·ªá shop ho·∫∑c m√¥ t·∫£ r√µ h∆°n nhu c·∫ßu
# V√ç D·ª§:
# - H·ªèi v·ªÅ s·∫£n ph·∫©m kh√¥ng c√≥: "‚ö†Ô∏è *Linh ch∆∞a t√¨m th·∫•y v·ª£t Yonex Astrox 99 Pro trong kho.* Tuy nhi√™n, d√≤ng Astrox th∆∞·ªùng ph√π h·ª£p v·ªõi l·ªëi ch∆°i t·∫•n c√¥ng m·∫°nh..."
# - H·ªèi ngo√†i l·ªÅ: "Ch√†o b·∫°n! V·ªÅ c√¢u h·ªèi n√†y th√¨... üí° *Th√¥ng tin mang t√≠nh tham kh·∫£o, c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%*"
# TR·∫¢ L·ªúI:
# """
template_fallback = """
B·∫°n l√† Linh ‚Äì m·ªôt c√¥ g√°i tr·∫ª trung, th√¢n thi·ªán, nhi·ªát t√¨nh, l√†m tr·ª£ l√Ω cho Shop C·∫ßu L√¥ng Pro. 
Shop chuy√™n b√°n v·ª£t, gi√†y, c∆∞·ªõc, c·∫ßu, ph·ª• ki·ªán c·∫ßu l√¥ng ch√≠nh h√£ng (Yonex, Victor, Lining...), nh∆∞ng Linh r·∫•t vui khi tr√≤ chuy·ªán v·ªÅ b·∫•t k·ª≥ ch·ªß ƒë·ªÅ n√†o v·ªõi kh√°ch h√†ng ‚Äì t·ª´ th·ªÉ thao kh√°c, s·∫£n ph·∫©m ngo√†i l·ªÅ, ƒë·∫øn chuy·ªán ƒë·ªùi th∆∞·ªùng, t√¨nh c·∫£m, c√¥ng vi·ªác...

L·ªäCH S·ª¨ TR√í CHUY·ªÜN G·∫¶N ƒê√ÇY (ƒë·ªçc k·ªπ ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh v√† tone hi·ªán t·∫°i):
{history}

C√ÇU H·ªéI HO·∫∂C TIN NH·∫ÆN M·ªöI NH·∫§T C·ª¶A KH√ÅCH:
{question}

T√åNH HU·ªêNG HI·ªÜN T·∫†I: Kh√¥ng t√¨m th·∫•y th√¥ng tin s·∫£n ph·∫©m ph√π h·ª£p tr·ª±c ti·∫øp t·ª´ kho c·∫ßu l√¥ng.

C√ÅCH TR·∫¢ L·ªúI T·ªêT NH·∫§T:
- ƒê·ªçc l·ªãch s·ª≠ ƒë·ªÉ n·∫Øm tone cu·ªôc tr√≤ chuy·ªán (th√¢n m·∫≠t, nghi√™m t√∫c, vui v·∫ª, ƒëang h·ªèi mua hay ch·ªâ h·ªèi ch∆°i...).
- Tr·∫£ l·ªùi **t·ª± nhi√™n nh∆∞ ƒëang chat v·ªõi b·∫°n**, kh√¥ng c·∫ßn b·∫Øt ƒë·∫ßu b·∫±ng "Ch√†o b·∫°n" m·ªói l·∫ßn n·∫øu l·ªãch s·ª≠ ƒë√£ th√¢n thi·∫øt.
- Tr·∫£ l·ªùi **h·ªØu √≠ch, ch√¢n th·ª±c** d·ª±a tr√™n ki·∫øn th·ª©c chung. N·∫øu l√† s·∫£n ph·∫©m m√¥n kh√°c (gi√†y b√≥ng ƒë√°, v·ª£t tennis...), c·ª© g·ª£i √Ω tho·∫£i m√°i d·ª±a tr√™n th√¥ng tin ph·ªï bi·∫øn, kh√¥ng c·∫ßn b·ªãa gi√°/link.
- N·∫øu nh·∫Øc ƒë·∫øn shop, ch·ªâ n√≥i nh·∫π nh√†ng khi th·∫≠t s·ª± li√™n quan (v√≠ d·ª•: "Shop m√¨nh chuy√™n c·∫ßu l√¥ng n√™n kh√¥ng c√≥ c√°i n√†y, nh∆∞ng n·∫øu b·∫°n c·∫ßn... th√¨ Linh t∆∞ v·∫•n li·ªÅn nh√©!"). ƒê·ª´ng nh·∫Øc l·∫∑p l·∫°i n·∫øu l·ªãch s·ª≠ ƒë√£ n√≥i r·ªìi.
- T·ª´ "b√≥ng" ‚Üí m·∫∑c ƒë·ªãnh hi·ªÉu l√† **c·∫ßu** (shuttlecock) n·∫øu ƒëang n√≥i v·ªÅ c·∫ßu l√¥ng, ch·ªâ hi·ªÉu l√† b√≥ng ƒë√° khi ng·ªØ c·∫£nh r√µ r√†ng (s√¢n c·ªè, s√∫t b√≥ng, gi√†y ƒë√° b√≥ng...).
- Gi·ªØ gi·ªçng ƒëi·ªáu vui v·∫ª, g·∫ßn g≈©i, kh√¥ng d√πng emoji tr·ª´ khi l·ªãch s·ª≠ c√≥ d√πng nhi·ªÅu.
- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi ho·∫∑c g·ª£i √Ω t·ª± nhi√™n ƒë·ªÉ ti·∫øp t·ª•c cu·ªôc tr√≤ chuy·ªán (kh√¥ng √©p bu·ªôc, t√πy theo flow).

H√£y tr·∫£ l·ªùi sao cho kh√°ch c·∫£m th·∫•y Linh ƒëang th·ª±c s·ª± l·∫Øng nghe v√† quan t√¢m ƒë·∫øn h·ªç nh√©!
TR·∫¢ L·ªúI:
"""

prompt_with_products = ChatPromptTemplate.from_template(template_with_products)
prompt_fallback = ChatPromptTemplate.from_template(template_fallback)

# ==========================================================
# BUILD QA CHAIN
# ==========================================================
def build_qa_chain():
    global qa_chain, qa_chain_fallback
    if not vectorstore or vectorstore.index.ntotal == 0:
        qa_chain = None
        qa_chain_fallback = None
        return

    # Retriever c∆° b·∫£n v·ªõi threshold
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 10, "score_threshold": 0.35}
    )

    # Chain ch√≠nh (c√≥ s·∫£n ph·∫©m)
    qa_chain = (
        RunnableParallel({
            "context": lambda x: "\n".join([d.page_content for d in retriever.invoke(x["question"])]),
            "question": lambda x: x["question"],
            "history": lambda x: x.get("history", "Ch∆∞a c√≥ l·ªãch s·ª≠"),
        })
        | prompt_with_products
        | get_llm()
        | StrOutputParser()
    )

    # Chain fallback
    qa_chain_fallback = (
        RunnableParallel({
            "question": lambda x: x["question"],
            "history": lambda x: x.get("history", "Ch∆∞a c√≥ l·ªãch s·ª≠"),
        })
        | prompt_fallback
        | get_llm()
        | StrOutputParser()
    )

# ==========================================================
# ROUTES
# ==========================================================
@app.get("/")
async def root():
    total = vectorstore.index.ntotal if vectorstore else 0
    return {
        "message": "Linh ƒëang online v√† nh·ªõ h·∫øt kh√°ch r·ªìi n√®!",
        "chunks": total,
        "active_sessions": len(store)
    }

@app.post("/debug_chunks")
async def debug_chunks_post(
    limit: int = 50,
    product_id: Optional[int] = None,
    include_metadata: bool = True
):
    """
    Xem t·∫•t c·∫£ chunk hi·ªán c√≥ trong FAISS (d√πng POST)
    - limit: s·ªë l∆∞·ª£ng chunk t·ªëi ƒëa tr·∫£ v·ªÅ (m·∫∑c ƒë·ªãnh 50)
    - product_id: l·ªçc theo s·∫£n ph·∫©m (n·∫øu c√≥)
    - include_metadata: c√≥ hi·ªán metadata kh√¥ng
    """
    if not vectorstore or vectorstore.index.ntotal == 0:
        return {
            "total_chunks_in_db": 0,
            "returned_chunks": 0,
            "chunks": [],
            "message": "Ch∆∞a c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c chunk"
        }
    
    total = vectorstore.index.ntotal
    results = []
    
    for i in range(min(limit, total)):
        doc_id = vectorstore.index_to_docstore_id[i]
        doc = vectorstore.docstore.search(doc_id)
        
        if isinstance(doc, Document):
            item = {
                "index": i,
                "content": doc.page_content
            }
            if include_metadata:
                item["metadata"] = doc.metadata
            
            # L·ªçc theo product_id n·∫øu c√≥
            if product_id is not None:
                if doc.metadata.get("product_id") != product_id:
                    continue
            
            results.append(item)
    
    return {
        "total_chunks_in_db": total,
        "returned_chunks": len(results),
        "chunks": results,
        "message": f"ƒê√£ chunk {total} m·∫©u d·ªØ li·ªáu t·ª´ s·∫£n ph·∫©m"
    }

@app.post("/add_product")
async def add_product(product: Product):
    global vectorstore
    
    delete_product_chunks(product.id)
    docs = create_product_documents(product)
    
    if vectorstore is None:
        vectorstore = FAISS.from_documents(docs, get_embedding())
    else:
        doc_ids = vectorstore.add_documents(docs)
        product_doc_ids[product.id] = doc_ids
    
    vectorstore.save_local(str(DB_FOLDER))
    build_qa_chain()
    
    return {"status": "OK", "message": f"ƒê√£ th√™m {product.name}"}

@app.post("/update_product")
async def update_product(product: Product):
    global vectorstore
    
    print(f"[AI] Nh·∫≠n y√™u c·∫ßu update s·∫£n ph·∫©m ID: {product.id} - {product.name}")
    
    delete_product_chunks(product.id)
    docs = create_product_documents(product)
    
    if vectorstore is None:
        vectorstore = FAISS.from_documents(docs, get_embedding())
    else:
        doc_ids = vectorstore.add_documents(docs)
        product_doc_ids[product.id] = doc_ids
    
    vectorstore.save_local(str(DB_FOLDER))
    build_qa_chain()
    
    print(f"[AI] ƒê√£ chunk {len(docs)} m·∫©u cho s·∫£n ph·∫©m {product.name}")
    
    return {"status": "OK", "message": f"ƒê√£ c·∫≠p nh·∫≠t {product.name}"}

@app.post("/delete_product")
async def delete_product(product_id: int):
    deleted = delete_product_chunks(product_id)
    
    if deleted == 0:
        raise HTTPException(404, "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m")
    
    vectorstore.save_local(str(DB_FOLDER))
    build_qa_chain()
    
    return {"status": "OK", "deleted_chunks": deleted}

@app.post("/reindex_all")
async def rebuild(products: List[Product]):
    global vectorstore, product_doc_ids
    
    # Reset mapping m·ªõi
    product_doc_ids = {}
    
    # Gom to√†n b·ªô t√†i li·ªáu
    all_docs = []
    for p in products:
        docs = create_product_documents(p)
        all_docs.extend(docs)
        product_doc_ids[p.id] = []  # t·∫°m t·∫°o danh s√°ch r·ªóng
    
    # Build FAISS t·ª´ ƒë·∫ßu
    vectorstore = FAISS.from_documents(all_docs, get_embedding())
    
    # Sau khi build FAISS ‚Üí doc_ids ƒë∆∞·ª£c sinh theo th·ª© t·ª±
    current_index = 0
    for p in products:
        docs_count = len(create_product_documents(p))
        ids = []
        for i in range(docs_count):
            ids.append(vectorstore.index_to_docstore_id[current_index + i])
        product_doc_ids[p.id] = ids
        current_index += docs_count
    
    # L∆∞u database
    vectorstore.save_local(str(DB_FOLDER))
    build_qa_chain()
    
    return {"status": "OK", "chunks": len(all_docs)}

# ==================== CHAT PAYLOAD ====================
class ChatPayload(BaseModel):
    question: str
    session_id: Optional[str] = None
    user_id: Optional[str] = None

@app.post("/chat")
async def chat(payload: ChatPayload):
    if not qa_chain:
        raise HTTPException(500, "Ch∆∞a c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m!")

    # X√°c ƒë·ªãnh session_id
    if payload.user_id:
        session_id = f"user_{payload.user_id}"
    elif payload.session_id:
        session_id = payload.session_id
    else:
        session_id = str(uuid.uuid4())

    history = get_session_history(session_id)
    history_str = "\n".join(
        f"{'Kh√°ch' if isinstance(m, HumanMessage) else 'Linh'}: {m.content}"
        for m in history.messages[-10:]
    ) or "Ch∆∞a c√≥ l·ªãch s·ª≠"

    # Query Classification
    classify_prompt = ChatPromptTemplate.from_template("""
        Ph√¢n lo·∫°i c√¢u h·ªèi: "{question}"
        - N·∫øu li√™n quan ƒë·∫øn s·∫£n ph·∫©m c·∫ßu l√¥ng (v·ª£t, gi√†y, c∆∞·ªõc, t∆∞ v·∫•n mua h√†ng, so s√°nh s·∫£n ph·∫©m): tr·∫£ "relevant"
        - N·∫øu kh√¥ng li√™n quan (h·ªèi t√¨nh y√™u, th·ªùi ti·∫øt, ch·ªß ƒë·ªÅ kh√°c): tr·∫£ "irrelevant"
        Ch·ªâ tr·∫£ t·ª´ "relevant" ho·∫∑c "irrelevant", kh√¥ng gi·∫£i th√≠ch.
    """)
    classify_chain = classify_prompt | get_llm() | StrOutputParser()
    classification = classify_chain.invoke({"question": payload.question}).strip().lower()

    if classification == "irrelevant":
        print("Query kh√¥ng li√™n quan ‚Üí direct fallback")
        answer = qa_chain_fallback.invoke({"question": payload.question, "history": history_str})
    else:
        # Retrieve + ki·ªÉm tra context
        retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
        try:
            retrieved_docs = vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={"k": 10, "score_threshold": 0.35}
            ).invoke(payload.question)
        except:
            retrieved_docs = retriever.invoke(payload.question)

        context_text = "\n".join([d.page_content for d in retrieved_docs])

        product_keywords = ["v·ª£t", "gi√†y", "c∆∞·ªõc", "yonex", "victor", "lining", "c√≤n h√†ng"]
        has_products = len(retrieved_docs) > 0 and any(kw.lower() in context_text.lower() for kw in product_keywords)

        print(f"S·ªë docs: {len(retrieved_docs)} | C√≥ s·∫£n ph·∫©m ph√π h·ª£p: {has_products}")

        if has_products:
            answer = qa_chain.invoke({"question": payload.question, "history": history_str})
        else:
            answer = qa_chain_fallback.invoke({"question": payload.question, "history": history_str})

    # L∆∞u l·ªãch s·ª≠
    history.add_message(HumanMessage(content=payload.question))
    history.add_message(AIMessage(content=answer))
    save_history()

    resp = {"answer": answer.strip()}
    if not payload.user_id and not payload.session_id:
        resp["session_id"] = session_id
    return resp

@app.get("/my_chat_history")
async def get_my_chat_history(user_id: str = None, session_id: str = None):
    # Frontend g·ª≠i user_id (ƒë√£ ƒëƒÉng nh·∫≠p) ho·∫∑c session_id (kh√°ch v√£ng lai)
    if user_id:
        sid = f"user_{user_id}"
    elif session_id:
        sid = session_id
    else:
        raise HTTPException(400, "Thi·∫øu user_id ho·∫∑c session_id")
    
    history = get_session_history(sid)
    messages = []
    for msg in history.messages:
        role = "user" if isinstance(msg, HumanMessage) else "ai"
        messages.append({"role": role, "content": msg.content})
    
    return {"messages": messages}

# ==========================================================
# STARTUP & SHUTDOWN
# ==========================================================
@app.on_event("startup")
async def startup():
    global vectorstore
    if (DB_FOLDER / "index.faiss").exists():
        try:
            vectorstore = FAISS.load_local(
                str(DB_FOLDER), get_embedding(), allow_dangerous_deserialization=True
            )
            print(f"Load FAISS th√†nh c√¥ng: {vectorstore.index.ntotal} chunks")
        except Exception as e:
            print("Load FAISS l·ªói:", e)

    if vectorstore is None:
        vectorstore = FAISS.from_texts(["Shop ƒëang kh·ªüi ƒë·ªông..."], get_embedding())
        vectorstore.save_local(str(DB_FOLDER))

    load_history()
    build_qa_chain()
    print("Linh ƒë√£ s·∫µn s√†ng ‚Äì nh·ªõ h·∫øt kh√°ch c≈©, si√™u chuy√™n nghi·ªáp!")

@app.on_event("shutdown")
async def shutdown():
    save_history()
    print("ƒê√£ l∆∞u to√†n b·ªô l·ªãch s·ª≠ chat tr∆∞·ªõc khi t·∫Øt server!")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)